# Configuration variables
DATASET_NAME="MNIST_pixano_v6"
customLearnerCondaEnv="customLearner2"

# variables that could be defined 
labels_per_round=100
round = 0 # current round
learning_rate=0.001
max_epochs_per_round=100
model_name="mlp" 
strategy="AlphaMixSampling" #EntropySampling #RandomSampling
alpha_opt=True


import os
import sys
import pandas as pd

def insertRootDir(ROOTDIR='pixano'):
    pardir=os.path.dirname(os.path.realpath('__file__'))

    while(os.path.basename(pardir)!=ROOTDIR):

        print(pardir)
        pardir=os.path.dirname(pardir)
        # print(os.path.basename(pardir))
    print("Inserting parent dir : ",pardir)
    sys.path.insert(0,pardir)
    return pardir

ROOTDIR = insertRootDir()


from pathlib import Path
from pixano.data import ImageImporter


library_dir=Path('/home/melissap/_pixano_datasets_') # directory where we have install the pixano formatted dataset
import_dir = library_dir / DATASET_NAME


# TAKEN FROM THE MNIST.ipynb notebook
# output path for lance database
DB_PATH = library_dir / "_launce_datasets_/MNIST"
# input image path
IMG_PATH = import_dir / "media"
# Note: images have been generated by MNIST (v1) notebook, and moved here
# TODO add a cell to generate image from mnist (from keras.datasets import mnist)

mnist_importer = ImageImporter("MNIST", "MNIST dataset for AL", ["train", "test"])
mnist_importer.import_dataset(
    input_dirs={ "image": IMG_PATH },
    import_dir=DB_PATH,
    portable=True
)

import random
import lancedb
import pyarrow as pa
import numpy as np
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.datasets import mnist
from ALearner import (
    Learner,
    BaseAnnotator,
    BaseSampler,
    BaseTrainer,
    getLabels,
    getLabelledIds,
    getUnlabelledIds,
    getTaggedIds,
    getLastRound,
    ddb_str,
    custom_update
)
from pixano.utils import natural_key

# utility function to convert id (format "<index>.png") to index
def id_to_idx(id: str) -> int:
    return int(id.split(".")[0])
    # return int(id[0:-4])  #remove the last 4 chars (".png")


mnist_db = lancedb.connect(import_dir)


(X_train, Y_train), (X_test, Y_test) = mnist.load_data()
num_pixels = X_train.shape[1] * X_train.shape[2]

def reshape_Xdata(x):
    #flatten images
    x = x.reshape(x.shape[0], num_pixels)
    #Convert to float
    x = x.astype('float32')
    #Normalize inputs from [0; 255] to [0; 1]
    x = x / 255
    return x

def reshape_Ydata(y):
    #Convert class vectors to binary class matrices ("one hot encoding")
    ## Doc : https://keras.io/utils/#to_categorical
    return keras.utils.to_categorical(y, num_classes=10)  # need to specify num_classes because sampler can miss some classes


#x_train = reshape_Xdata(X_train)
x_test = reshape_Xdata(X_test)
y_train = reshape_Ydata(Y_train)
y_test = reshape_Ydata(Y_test)
num_classes = y_train.shape[1]

def neural_network():
    model = Sequential()
    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))
    #model.add(Dropout(.5))
    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model = neural_network()


class SimpleTrainer(BaseTrainer):
    # simple trainer, train on all labeled data
    def __init__(self, db, model, validation_data, avoid_overfit=False):
        self.init_weights = model.get_weights()
        self.avoid_overfit = avoid_overfit
        super().__init__(db, model, validation_data)

    # training on subset data
    def train(self, epochs, batch_size):
        # get y data (labels) and ids from db, x data (images) from raw mnist and ids
        ids = getLabelledIds(self.db)
        labels = getLabels(self.db)
        if self.avoid_overfit:
            print("Reset weights to avoid overfit")
            self.model.set_weights(self.init_weights)
        print(f"Train on {len(ids)} labelled items")
        x_train = reshape_Xdata(np.array([X_train[id_to_idx(id)] for id in ids]))
        y_train = reshape_Ydata(np.array(labels))
        self.model.fit(x_train, y_train, validation_data=self.validation_data, epochs=epochs, batch_size=batch_size)
        scores = model.evaluate(self.validation_data[0], self.validation_data[1])
        print("Neural network accuracy: %.2f%%" % (scores[1]*100))
        return {
            "score": scores[1]*100
        }

class IncrementalTrainer(BaseTrainer):
    #in this trainer we train only on last round
    def __init__(self, db, model, validation_data):
        self.initial_epoch = 0
        super().__init__(db, model, validation_data)

    # training on subset data
    def train(self, epochs, batch_size):
        # get y data (labels) and ids from db, x data (images) from raw mnist and ids
        round = getLastRound(self.db)
        ids = getLabelledIds(self.db, round)
        labels = getLabels(self.db, round)
        print(f"Train on {len(ids)} labelled items. initial epoch = {self.initial_epoch}")
        x_train = reshape_Xdata(np.array([X_train[id_to_idx(id)] for id in ids]))
        y_train = reshape_Ydata(np.array(labels))
        self.model.fit(x_train, y_train, validation_data=self.validation_data, epochs=self.initial_epoch+epochs, batch_size=batch_size, initial_epoch=self.initial_epoch)
        scores = model.evaluate(self.validation_data[0], self.validation_data[1])
        print("Neural network accuracy: %.2f%%" % (scores[1]*100))
        # update initial_epoch for next round
        self.initial_epoch += epochs
        return {
            "score": scores[1]*100
        }


# input: ids (whole dataset, or filtered (here: train only))
# output: candidates

class RandomSampler(BaseSampler):

    def query(self, n_candidates=10):
        ids = getUnlabelledIds(self.db, split="train")
        return random.sample(ids, n_candidates)

class SequentialSampler(BaseSampler):

    def query(self, n_candidates=10):
        ids = getUnlabelledIds(self.db, split="train")
        return sorted(ids, key=int)[0:n_candidates]

###########################################################################################################################################################
import subprocess

# TEMPORARY SOLUTION
def create_dir(path):
    try:
        os.makedirs(path)
    except:
        print(f'Dir {path} exists already')
    return path
# here define the paths of exchanging data between pixano and the customLearner
temp_data_exchange_dir = create_dir(os.path.join(ROOTDIR,"temp_data"))              # define a directory for exchanging data
dataset = str(import_dir)                                                           # [in] frames
output_queDir=create_dir(os.path.join(temp_data_exchange_dir,"output_queries"))        # [in] annotations passed to the learner
output_accDir=create_dir(os.path.join(temp_data_exchange_dir,"output_accuracy"))      # [out] results obtained from the learner


class customTrainer():

    weights_dir="_weights"
    # batch_size = 16
    learning_rate=0.001
    n_epoch=100
    model="mlp" 
    
    mode='train'

    #in this trainer we train only on last round
    def __init__(self, db, model, validation_data, **kwargs):
        self.db = db
        self.validation_data = validation_data # ---------------------------> remove later
        self.initial_epoch = 0

        # sets new values to any default arguments passed during construction    
        for key, value in kwargs.items():
            if hasattr(self, key):
                self.set_parameter(key,value)
    
    def set_parameter(self,key,value):
        # change member variable members. Public method that can be used outside the scope of the scope
        if hasattr(self, key):
            setattr(self, key, value)
        else:
            print(f'Argument {key} does not exist. Value of {value} does not set any of the member values of the customTrainer class')

    # training on subset data
    def train(self, epochs, batch_size):

        curRound = getLastRound(self.db)
        # csvAcc=os.path.join(output_accDir,"accuracy"+str(curRound)+".csv")
        csvAcc=os.path.join(output_accDir,"accuracy.csv")

        arguments = f"--data_name {DATASET_NAME} --mode {self.mode} --mode train --train_out {csvAcc} --data_dir {import_dir} --n_query {labels_per_round} --learning_rate {learning_rate} --n_epoch {max_epochs_per_round} --model {model_name} --strategy {strategy} --alpha_opt"
        # arguments = f"--data_name {DATASET_NAME} --mode {self.mode} --mode train --train_out {csvAcc} --data_dir {import_dir} --n_query {labels_per_round} --learning_rate {learning_rate} --n_epoch 100 --model {model} --strategy {strategy} --alpha_opt"
        subprocess.run(f"""source ~/miniconda3/etc/profile.d/conda.sh
            conda activate {customLearnerCondaEnv} 
            python alpha_mix_active_learning/_main.py {arguments}""", #{customLearner_ROOTDIR}/customLearner_main_3
            shell=True, executable='/bin/bash', check=True)

        trainOut = pd.read_csv(csvAcc,index_col=0)
        return {
            "score": 100 * trainOut.loc["round_"+str(curRound),"accuracy"]
        }
        
# here define the implementation for the new sampler
class customSampler(BaseSampler):
    
    #add all other dependencies define in https://docs.google.com/document/d/1NlArhWYjePzB43sR4HCUc_4xBU73Up9OI24hIyPx0zY/edit

    # for now only the vital ones
    output_dir="_output"
    log_directory="_logs"
    n_init_lb=100
    n_query=100 
    alpha_opt=True
    mode = "query"

    def __init__(self, dataset, **kwargs):
        super().__init__(dataset)

        # sets new values to any default arguments passed during construction    
        for key, value in kwargs.items():
            if hasattr(self, key):
                self.set_parameter(key,value)

    def set_parameter(self,key,value):
        # change member variable members. Public method that can be used outside the scope of the scope
        if hasattr(self, key):
            setattr(self, key, value)
        else:
            print(f'Argument {key} does not exist. Value of {value} does not set any of the member values of the customSampler class')

    def query(self, discard_n_candidates=10):
        # under active development
        round = getLastRound(self.db)

        # if (round == -1):                                                   # random sampling when labels are absent
        #     ids = getLabelledIds(self.db, round)
        #     return random.sample(ids, labels_per_round)
        # elif (round >= 0):
        curRound = getLastRound(self.db)

        csvQue=os.path.join(output_queDir,"queries_"+str(curRound)+".csv")

        arguments = f"--data_name {DATASET_NAME} --data_dir {import_dir} --mode {self.mode} --query_out {csvQue} --n_query {labels_per_round} --model {model_name} --strategy {strategy} --alpha_opt"
        # arguments = f"--data_name {DATASET_NAME} --data_dir {import_dir} --mode {self.mode} --query_out {csvQue} --n_query {labels_per_round} --model {model} --strategy {strategy} --alpha_opt"
        subprocess.run(f"""source ~/miniconda3/etc/profile.d/conda.sh
                    conda activate {customLearnerCondaEnv} 
                    python alpha_mix_active_learning/_main.py {arguments}""",
                    shell=True, executable='/bin/bash', check=True)
        
        queryOut = pd.read_csv(csvQue,index_col=0)
        
        return queryOut["query_results"].tolist()
            
class AutoAnnotator(BaseAnnotator):
    # custom annotation function
    # as we have ground truth for MNIST, we can autofill
    def annotate(self, round):
        candidates = getTaggedIds(self.db, round)
        db_tbl = mnist_db.open_table("db")
        custom_update(db_tbl, f"id in ({ddb_str(candidates)})", 'label', [str(Y_train[id_to_idx(candidate)]) for candidate in sorted(candidates, key=natural_key)])
        print(f"AutoAnnotator: round {round} annotated.")



# train on all data, don't reset weights before each training round (it may overfit on first rounds data)
# myTrainer = SimpleTrainer(mnist_db, model, (x_test, y_test))

# train on all data, reset weights before each training round
#myTrainer = SimpleTrainer(mnist_db, model, (x_test, y_test), avoid_overfit=True) 

# train on candidates data (without resetting weights obviously)
# myTrainer = IncrementalTrainer(mnist_db, model, (x_test, y_test))
myTrainer = customTrainer(mnist_db, model, (x_test, y_test))

# randomSampler = RandomSampler(mnist_db)
randomSampler = customSampler(mnist_db, n_query=200, irrelevant=5)

autofillAnnotator = AutoAnnotator(mnist_db)

epochs = 1

init_learner = Learner(
    db=mnist_db,
    trainer=myTrainer,
    sampler=randomSampler,
    custom_annotator=autofillAnnotator,
    new_al=True,
    verbose=0
)

# candidates = init_learner.query(round, labels_per_round)
# init_learner.annotate(round)
# init_learner.train(round, epochs=epochs)

# # round += 1

# candidates = init_learner.query(, labels_per_round)

auto_rounds = 3
# round_size = 20
for round in range(round, round+auto_rounds):
    candidates = init_learner.query(round, labels_per_round)
    init_learner.annotate(round)
    init_learner.train(round, epochs=epochs)