{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning with Pixano - MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"MNIST_pixano_v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/melissap/Desktop/LAGO/3.githubs/integration/2.integrateAL/pixano/dev/INTEGRATION/231031_mnist\n",
      "/home/melissap/Desktop/LAGO/3.githubs/integration/2.integrateAL/pixano/dev/INTEGRATION\n",
      "/home/melissap/Desktop/LAGO/3.githubs/integration/2.integrateAL/pixano/dev\n",
      "Inserting parent dir :  /home/melissap/Desktop/LAGO/3.githubs/integration/2.integrateAL/pixano\n"
     ]
    }
   ],
   "source": [
    "# import ROOT dir to import pixano root module\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def insertRootDir(ROOTDIR='pixano'):\n",
    "    pardir=os.path.dirname(os.path.realpath('__file__'))\n",
    "\n",
    "    while(os.path.basename(pardir)!=ROOTDIR):\n",
    "\n",
    "        print(pardir)\n",
    "        pardir=os.path.dirname(pardir)\n",
    "        # print(os.path.basename(pardir))\n",
    "    print(\"Inserting parent dir : \",pardir)\n",
    "    sys.path.insert(0,pardir)\n",
    "\n",
    "insertRootDir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pixano.data import ImageImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_dir=Path('/home/melissap/_pixano_datasets_')\n",
    "import_dir = library_dir / DATASET_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0655d59a4dba48bab2a66a456a51aa33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Importing dataset: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936168571ac64117b2ffbc44f04f6086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copying media directories:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d7e35940ec41adb257e60d02077f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating dataset info file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<pixano.data.dataset.Dataset at 0x7f6c05f50280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TAKEN FROM THE MNIST.ipynb notebook\n",
    "# output path for lance database\n",
    "DB_PATH = library_dir / \"_launce_datasets_/MNIST\"\n",
    "# input image path\n",
    "IMG_PATH = import_dir / \"media\"\n",
    "# Note: images have been generated by MNIST (v1) notebook, and moved here\n",
    "# TODO add a cell to generate image from mnist (from keras.datasets import mnist)\n",
    "\n",
    "mnist_importer = ImageImporter(\"MNIST\", \"MNIST dataset for AL\", [\"train\", \"test\"])\n",
    "mnist_importer.import_dataset(\n",
    "    input_dirs={ \"image\": IMG_PATH },\n",
    "    import_dir=DB_PATH,\n",
    "    portable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 11:23:38.435042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from ALearner import (\n",
    "    Learner,\n",
    "    BaseAnnotator,\n",
    "    BaseSampler,\n",
    "    BaseTrainer,\n",
    "    getLabels,\n",
    "    getLabelledIds,\n",
    "    getUnlabelledIds,\n",
    "    getTaggedIds,\n",
    "    getLastRound,\n",
    "    ddb_str,\n",
    "    custom_update\n",
    ")\n",
    "from pixano.utils import natural_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to convert id (format \"<index>.png\") to index\n",
    "def id_to_idx(id: str) -> int:\n",
    "    return int(id.split(\".\")[0])\n",
    "    # return int(id[0:-4])  #remove the last 4 chars (\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Pixano DB\n",
    "MNIST dataset should have been imported previously (see lance_importers/MNIST.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_db = lancedb.connect(import_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer Object\n",
    "\n",
    "We will get raw x_train, x_test, y_test data directly from MNIST.\n",
    "\n",
    "2 proposed Model Trainer Objects, with same model: SimpleTrainer and IncrementalTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 11:23:39.711406: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "\n",
    "def reshape_Xdata(x):\n",
    "    #flatten images\n",
    "    x = x.reshape(x.shape[0], num_pixels)\n",
    "    #Convert to float\n",
    "    x = x.astype('float32')\n",
    "    #Normalize inputs from [0; 255] to [0; 1]\n",
    "    x = x / 255\n",
    "    return x\n",
    "\n",
    "def reshape_Ydata(y):\n",
    "    #Convert class vectors to binary class matrices (\"one hot encoding\")\n",
    "    ## Doc : https://keras.io/utils/#to_categorical\n",
    "    return keras.utils.to_categorical(y, num_classes=10)  # need to specify num_classes because sampler can miss some classes\n",
    "\n",
    "\n",
    "#x_train = reshape_Xdata(X_train)\n",
    "x_test = reshape_Xdata(X_test)\n",
    "y_train = reshape_Ydata(Y_train)\n",
    "y_test = reshape_Ydata(Y_test)\n",
    "num_classes = y_train.shape[1]\n",
    "\n",
    "def neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dropout(.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = neural_network()\n",
    "\n",
    "\n",
    "class SimpleTrainer(BaseTrainer):\n",
    "    # simple trainer, train on all labeled data\n",
    "    def __init__(self, db, model, validation_data, avoid_overfit=False):\n",
    "        self.init_weights = model.get_weights()\n",
    "        self.avoid_overfit = avoid_overfit\n",
    "        super().__init__(db, model, validation_data)\n",
    "\n",
    "    # training on subset data\n",
    "    def train(self, epochs, batch_size):\n",
    "        # get y data (labels) and ids from db, x data (images) from raw mnist and ids\n",
    "        ids = getLabelledIds(self.db)\n",
    "        labels = getLabels(self.db)\n",
    "        if self.avoid_overfit:\n",
    "            print(\"Reset weights to avoid overfit\")\n",
    "            self.model.set_weights(self.init_weights)\n",
    "        print(f\"Train on {len(ids)} labelled items\")\n",
    "        x_train = reshape_Xdata(np.array([X_train[id_to_idx(id)] for id in ids]))\n",
    "        y_train = reshape_Ydata(np.array(labels))\n",
    "        self.model.fit(x_train, y_train, validation_data=self.validation_data, epochs=epochs, batch_size=batch_size)\n",
    "        scores = model.evaluate(self.validation_data[0], self.validation_data[1])\n",
    "        print(\"Neural network accuracy: %.2f%%\" % (scores[1]*100))\n",
    "        return {\n",
    "            \"score\": scores[1]*100\n",
    "        }\n",
    "\n",
    "class IncrementalTrainer(BaseTrainer):\n",
    "    #in this trainer we train only on last round\n",
    "    def __init__(self, db, model, validation_data):\n",
    "        self.initial_epoch = 0\n",
    "        super().__init__(db, model, validation_data)\n",
    "\n",
    "    # training on subset data\n",
    "    def train(self, epochs, batch_size):\n",
    "        # get y data (labels) and ids from db, x data (images) from raw mnist and ids\n",
    "        round = getLastRound(self.db)\n",
    "        ids = getLabelledIds(self.db, round)\n",
    "        labels = getLabels(self.db, round)\n",
    "        print(f\"Train on {len(ids)} labelled items. initial epoch = {self.initial_epoch}\")\n",
    "        x_train = reshape_Xdata(np.array([X_train[id_to_idx(id)] for id in ids]))\n",
    "        y_train = reshape_Ydata(np.array(labels))\n",
    "        self.model.fit(x_train, y_train, validation_data=self.validation_data, epochs=self.initial_epoch+epochs, batch_size=batch_size, initial_epoch=self.initial_epoch)\n",
    "        scores = model.evaluate(self.validation_data[0], self.validation_data[1])\n",
    "        print(\"Neural network accuracy: %.2f%%\" % (scores[1]*100))\n",
    "        # update initial_epoch for next round\n",
    "        self.initial_epoch += epochs\n",
    "        return {\n",
    "            \"score\": scores[1]*100\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Sampler Object\n",
    "<!-- RandomSampler or SequentialSampler -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: ids (whole dataset, or filtered (here: train only))\n",
    "# output: candidates\n",
    "\n",
    "class RandomSampler(BaseSampler):\n",
    "\n",
    "    def query(self, n_candidates=10):\n",
    "        ids = getUnlabelledIds(self.db, split=\"train\")\n",
    "        return random.sample(ids, n_candidates)\n",
    "\n",
    "class SequentialSampler(BaseSampler):\n",
    "\n",
    "    def query(self, n_candidates=10):\n",
    "        ids = getUnlabelledIds(self.db, split=\"train\")\n",
    "        return sorted(ids, key=int)[0:n_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Interface Objects\n",
    "\n",
    "Human labeling with Pixano Annotator is built-in, here we specify an Auto Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoAnnotator(BaseAnnotator):\n",
    "    # custom annotation function\n",
    "    # as we have ground truth for MNIST, we can autofill\n",
    "    def annotate(self, round):\n",
    "        candidates = getTaggedIds(self.db, round)\n",
    "        db_tbl = mnist_db.open_table(\"db\")\n",
    "        custom_update(db_tbl, f\"id in ({ddb_str(candidates)})\", 'label', [str(Y_train[id_to_idx(candidate)]) for candidate in sorted(candidates, key=natural_key)])\n",
    "        print(f\"AutoAnnotator: round {round} annotated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on all data, don't reset weights before each training round (it may overfit on first rounds data)\n",
    "# myTrainer = SimpleTrainer(mnist_db, model, (x_test, y_test))\n",
    "\n",
    "# train on all data, reset weights before each training round\n",
    "#myTrainer = SimpleTrainer(mnist_db, model, (x_test, y_test), avoid_overfit=True) \n",
    "\n",
    "# train on candidates data (without resetting weights obviously)\n",
    "myTrainer = IncrementalTrainer(mnist_db, model, (x_test, y_test))\n",
    "\n",
    "randomSampler = RandomSampler(mnist_db)\n",
    "autofillAnnotator = AutoAnnotator(mnist_db)\n",
    "\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0 tagged\n",
      "57 candidates on round 0\n",
      "AutoAnnotator: round 0 annotated.\n",
      "Train on 71 labelled items. initial epoch = 0\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2.3302 - accuracy: 0.1408 - val_loss: 2.4446 - val_accuracy: 0.0966\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 2.4446 - accuracy: 0.0966\n",
      "Neural network accuracy: 9.66%\n"
     ]
    }
   ],
   "source": [
    "init_round_size = 50\n",
    "round = 0\n",
    "\n",
    "init_learner = Learner(\n",
    "    db=mnist_db,\n",
    "    trainer=myTrainer,\n",
    "    sampler=randomSampler,\n",
    "    custom_annotator=autofillAnnotator,\n",
    "    new_al=True,\n",
    "    verbose=0\n",
    ")\n",
    "candidates = init_learner.query(round, init_round_size)\n",
    "init_learner.annotate(round)\n",
    "init_learner.train(round, epochs=epochs)\n",
    "\n",
    "round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 tagged\n"
     ]
    }
   ],
   "source": [
    "candidates = init_learner.query(round, init_round_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add some auto-annotation rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 tagged\n",
      "85 candidates on round 1\n",
      "AutoAnnotator: round 1 annotated.\n",
      "Train on 115 labelled items. initial epoch = 1\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 2.4600 - accuracy: 0.1217 - val_loss: 2.4518 - val_accuracy: 0.0591\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 2.4518 - accuracy: 0.0591\n",
      "Neural network accuracy: 5.91%\n",
      "Round 2 tagged\n",
      "21 candidates on round 2\n",
      "AutoAnnotator: round 2 annotated.\n",
      "Train on 23 labelled items. initial epoch = 2\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 576ms/step - loss: 2.4414 - accuracy: 0.1739 - val_loss: 2.4852 - val_accuracy: 0.0710\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 2.4852 - accuracy: 0.0710\n",
      "Neural network accuracy: 7.10%\n",
      "Round 3 tagged\n",
      "23 candidates on round 3\n",
      "AutoAnnotator: round 3 annotated.\n",
      "Train on 29 labelled items. initial epoch = 3\n",
      "Epoch 4/4\n",
      "1/1 [==============================] - 1s 682ms/step - loss: 2.4717 - accuracy: 0.1034 - val_loss: 2.5066 - val_accuracy: 0.0840\n",
      "313/313 [==============================] - 4s 11ms/step - loss: 2.5066 - accuracy: 0.0840\n",
      "Neural network accuracy: 8.40%\n"
     ]
    }
   ],
   "source": [
    "auto_rounds = 3\n",
    "round_size = 20\n",
    "for round in range(round, round+auto_rounds):\n",
    "    candidates = init_learner.query(round, round_size)\n",
    "    init_learner.annotate(round)\n",
    "    init_learner.train(round, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Learning - Human annotation with Pixano Annotator\n",
    "\n",
    "Here we use a different Learner for human annotation. Trainer Object use the same model so we keep training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4 tagged\n",
      "4 candidates on round 4\n",
      "4 items to annotate on round 4: ['8127.jpeg', '17773.jpeg', '36560.jpeg', '8127.jpeg']\n",
      "Interrupted, current round has been canceled, and round labels erased\n"
     ]
    }
   ],
   "source": [
    "pix_rounds = 2\n",
    "pix_round_size = 3\n",
    "learner_pix = Learner(\n",
    "    db=mnist_db,\n",
    "    trainer=myTrainer,\n",
    "    sampler=randomSampler\n",
    ")\n",
    "for round in range(round + 1, pix_rounds + round + 1):\n",
    "    candidates = learner_pix.query(round, pix_round_size)\n",
    "    # if aborted, we must untag the current round \n",
    "    try:\n",
    "        learner_pix.annotate(round)\n",
    "    except KeyboardInterrupt:\n",
    "        learner_pix.untagRound(round)\n",
    "        round = round - 1\n",
    "        print(\"Interrupted, current round has been canceled, and round labels erased\")\n",
    "        break\n",
    "    result = learner_pix.train(round, epochs=epochs)\n",
    "    print(\"result\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('pixano')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6604ca948f4575fb1939f96f2cd7df5de428f245d6855ef61050d788c6a2a026"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
